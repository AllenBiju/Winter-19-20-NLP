{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "import nltk\n",
    "f = open('C://Users//Allen Biju Thomas//Desktop//MASC-3.0.0//MASC-3.0.0//data//written//twitter//tweets1.txt','r')\n",
    "text = f.read()     # read from file\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['just', 'setting', 'up', 'my', 'twttr', 'tm', 'help', 'Wondering', 'if', 'I']\n",
      "<Text: just setting up my twttr tm help Wondering...>\n",
      "Displaying 6 of 6 matches:\n",
      "just setting up my twttr tm help Wondering if I should quit work and become a\n",
      "! Too bad it's so inhospitable. is wondering how the hell she's going to write \n",
      "ng shade in the warm Danville sun, wondering if I'll see any more wild turkeys \n",
      "pathology book,listening music and wondering what movie is the one running on t\n",
      "Comes from Don McLean song \"An ... wondering what I should do with this twitter\n",
      "ert! At lunch sitting on the couch wondering what this whole twitter things is \n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "text1 = text.split()\n",
    "print(text1[:10])\n",
    "text2 = nltk.Text(text1)    #convert to nltk readble text\n",
    "print(text2)\n",
    "text2.concordance(\"wondering\")      #show word in context\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "257727\n",
      "['\\ufeffThe', 'Project', 'Gutenberg', 'EBook', 'of', 'Crime', 'and', 'Punishment', ',', 'by', 'Fyodor', 'Dostoevsky', 'This', 'eBook', 'is', 'for', 'the', 'use', 'of', 'anyone']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "from urllib import request\n",
    "url = \"https://www.gutenberg.org/files/2554/2554-0.txt\"\n",
    "response = request.urlopen(url)     # Web request for the url\n",
    "raw =  response.read().decode('utf-8')\n",
    "\n",
    "from nltk.tokenize import  word_tokenize\n",
    "tokens = word_tokenize(raw)     # split to tokens\n",
    "print(len(tokens))\n",
    "print(tokens[:20])\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "file_extension": ".py",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
